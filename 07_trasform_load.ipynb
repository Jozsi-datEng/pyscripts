{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 30)\n",
    "import requests\n",
    "%run \"00_common_res.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "implementation plan\n",
    "\n",
    "1. get data from S3\n",
    "2. wheater data transfommations\n",
    "3. taxi data transfommations *** done\n",
    "4. update payment types table *** done\n",
    "5. update companies table *** done\n",
    "6. update taxi data with company and payment types ids (replace strings with id from latest tables)  *** done\n",
    "7-8. upload taxi and weather data to s3\n",
    "9. upload new company and payment types table to s3\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 1 day trip data from protal by API\n",
    "url_chportal_taxi_api = (f\"https://data.cityofchicago.org/resource/ajtu-isnz.json?\"\n",
    "    f\"$where=trip_start_timestamp >= '{formatted_date}T00:00:00' \"\n",
    "    f\"AND trip_start_timestamp <= '{formatted_date}T23:59:59'&$limit=30000\")\n",
    "res_chportal_api = requests.get(url_chportal_taxi_api)\n",
    "# format to json\n",
    "js_taxi_data = res_chportal_api.json()\n",
    "# convert json format data to dataFrame\n",
    "df_taxi_data =  pd.DataFrame (js_taxi_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trasfomation part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### taxi data transformation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# typing help fir dict creation:-)\n",
    "df_col_types = df_taxi_data.dtypes\n",
    "for s_colname in df_col_types.keys():\n",
    "    print(f'\"{s_colname}\": \"{df_col_types[s_colname]}\",')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def taxi_data_transformations(df_taxi_data):\n",
    "    \"\"\" taxi_data_transformations\n",
    "    ver: v1.1\n",
    "    Params:\n",
    "        df_taxi_data (df): original daly taxi data\n",
    "    Returns:\n",
    "        df: cleaned & trasform taxi data\n",
    "    requirements:\n",
    "        module(s): pandas\n",
    "    \"\"\"\n",
    "    # drop unnecessary columns\n",
    "    df_taxi_data.drop([\"pickup_census_tract\", \"dropoff_census_tract\"], axis=1, inplace=True)\n",
    "    df_taxi_data.drop([\"pickup_centroid_location\", \"dropoff_centroid_location\"], axis=1, inplace=True)\n",
    "\n",
    "    # drop rows that have missings\n",
    "    df_taxi_data.dropna (inplace=True)\n",
    "\n",
    "    # renaming cols\n",
    "    di_col_old_new = {\n",
    "        \"pickup_community_area\": \"pickup_community_area_id\", \"dropoff_community_area\": \"dropoff_community_area_id\"}\n",
    "    df_taxi_data.rename (columns=di_col_old_new, inplace=True)\n",
    "\n",
    "    # create helper column\n",
    "    df_taxi_data[\"datetime_for_weather\"] = pd.to_datetime(df_taxi_data[\"trip_start_timestamp\"])\n",
    "\n",
    "    # time rounding\n",
    "    df_taxi_data[\"datetime_for_weather\"] = df_taxi_data[\"datetime_for_weather\"].dt.floor(\"h\")\n",
    "\n",
    "    # create dict with types def\n",
    "    di_df_col_type = {\n",
    "        \"trip_id\": \"object\",\n",
    "        \"taxi_id\": \"object\",\n",
    "        \"trip_start_timestamp\": \"datetime64[ns]\",\n",
    "        \"trip_end_timestamp\": \"datetime64[ns]\",\n",
    "        \"trip_seconds\": \"int32\",\n",
    "        \"trip_miles\": \"float64\",\n",
    "        \"pickup_community_area_id\": \"int8\",\n",
    "        \"dropoff_community_area_id\": \"int8\",\n",
    "        \"fare\": \"float64\",\n",
    "        \"tips\": \"float64\",\n",
    "        \"tolls\": \"float64\",\n",
    "        \"extras\": \"float64\",\n",
    "        \"trip_total\": \"float64\",\n",
    "        \"payment_type\": \"object\",\n",
    "        \"company\": \"object\",\n",
    "        \"pickup_centroid_latitude\": \"object\",\n",
    "        \"pickup_centroid_longitude\": \"object\",\n",
    "        \"dropoff_centroid_latitude\": \"object\",\n",
    "        \"dropoff_centroid_longitude\": \"object\",\n",
    "        \"datetime_for_weather\": \"datetime64[ns]\"\n",
    "    }\n",
    "    # apply to dataFrame\n",
    "    df_taxi_data = df_taxi_data.astype(di_df_col_type)\n",
    "    return df_taxi_data\n",
    "\n",
    "taxi_data_transformations(df_taxi_data)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### update master table function develop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def master_table_update(df_master, df_dim, s_id_name, s_column_label):\n",
    "    \"\"\" compare master and dim table. Expand master with new items if necessary.\n",
    "    ver: 1.0\n",
    "    Params:\n",
    "        df_master (df): master data table\n",
    "        df_dim (df): Actual dim (extract) table from taxi data\n",
    "        s_id_name (str): master table id column name\n",
    "        s_column_label (str): master table label column name\n",
    "    Returns:\n",
    "        df: extended master table with new item(s)\n",
    "    requirements:\n",
    "        module(s): pandas\n",
    "    \"\"\"\n",
    "    s_my_name = 'master_table_update'\n",
    "    # we compare them with sets form\n",
    "    se_dim = set(df_dim[s_column_label].to_list())\n",
    "    se_master = set(df_master[s_column_label].to_list())\n",
    "    # make an additive list\n",
    "    ls_dim = list(se_dim - se_master)\n",
    "    # if there is'nt new element(s), return with original dataFrame\n",
    "    if not ls_dim:\n",
    "        print(f'Function {s_my_name}: No new element was added to the master table')\n",
    "        return df_master\n",
    "    # calc new id list\n",
    "    ls_master_id = list(range(len(df_master)+1,len(df_master) + len(ls_dim)+1))\n",
    "    # create a dict with the lists\n",
    "    di_company_add = {s_id_name: ls_master_id, s_column_label: ls_dim}\n",
    "    # put a dataFrame\n",
    "    df_add = pd.DataFrame(di_company_add)\n",
    "    # concat them\n",
    "    df_master = pd.concat([df_master, df_add], ignore_index=True)\n",
    "    print(f'Function {s_my_name}: master table was updated!')\n",
    "    return df_master\n",
    "\n",
    "# alternative comparison\n",
    "# pd.merge(df_companies_new, df_companies['company'], on= 'company', how='left', indicator=True).query('_merge == \"left_only\"').drop('_merge', axis=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create simulation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define column & dim table names to generate\n",
    "ls_dim_names = [\n",
    "        ['payment_type', 'df_payment_types_new'],\n",
    "        ['company', 'df_companies_new']\n",
    "    ]\n",
    "\n",
    "# craete dim tables\n",
    "for ls_dim_names_row in ls_dim_names:\n",
    "    # generate unique extracts\n",
    "    df_dim_tmp = df_taxi_data[ls_dim_names_row[0]].drop_duplicates()\n",
    "    df_dim_tmp = df_dim_tmp.sort_values(ignore_index=True)\n",
    "\n",
    "    # create dim table\n",
    "    df_dim_tmp = pd.DataFrame(\n",
    "        {\n",
    "            f'{ls_dim_names_row[0]}_id': range(1, len(df_dim_tmp)+1), # create range for ID\n",
    "            ls_dim_names_row[0]: df_dim_tmp\n",
    "\n",
    "        }\n",
    "    )\n",
    "    # rename dataFrame\n",
    "    globals()[ls_dim_names_row[1]] = df_dim_tmp\n",
    "\n",
    "    \n",
    "    ## save\n",
    "    # s_path_dim = s_path_sep.join([s_base_path, s_dir_data, ls_dim_names_row[1]+'.csv'])\n",
    "    # df_export_to_csv(df_dim_tmp, s_path_dim, s_dir_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat the end of master table, to simulate miss\n",
    "df_companies = df_companies_new.iloc[0:24]\n",
    "# make a new table that simulate actual dim status\n",
    "df_companies_new = df_companies_new.drop([0,1,2]).reset_index(drop=True)\n",
    "df_companies_new = df_companies_new.drop('company_id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat the end of master table, to simulate miss\n",
    "df_payment_types = df_payment_types_new.iloc[0:3]\n",
    "# make a new table that simulate actual dim status\n",
    "df_payment_types_new = df_payment_types_new.drop([0,1]).reset_index(drop=True)\n",
    "df_payment_types_new = df_payment_types_new.drop('payment_type_id', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### function testings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test companies\n",
    "df_companies = master_table_update(df_master=df_companies, df_dim=df_companies_new, s_id_name='company_id', s_column_label='company')\n",
    "df_companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test payment_types\n",
    "df_payment_types = master_table_update(df_master=df_payment_types, df_dim=df_payment_types_new, s_id_name='payment_type_id', s_column_label='payment_type')\n",
    "df_payment_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### update taxi data with actual master tables, replace labels to id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_label_to_master_id(df_data, df_master, s_join_colname):\n",
    "    \"\"\" join master table id to df_data and remove original label column\n",
    "    ver: 1.0\n",
    "    Params:\n",
    "        df_data (df): main dataFrame\n",
    "        df_master (df): master table to join\n",
    "        s_join_colname (str): join on column name\n",
    "    Returns:\n",
    "        df: main dataFrame with new id a removed label columns\n",
    "    requirements:\n",
    "        module(s): pandas\n",
    "    \"\"\"\n",
    "    # join master tables data\n",
    "    df_data = df_data.merge(df_master, on=s_join_colname)\n",
    "    # drop label coulumns\n",
    "    df_data.drop([s_join_colname], axis=1, inplace=True)\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_taxi_data = df_taxi_data.merge(df_payment_types, on='payment_type')\n",
    "\n",
    "df_taxi_data = replace_label_to_master_id(df_data=df_taxi_data, df_master=df_companies, s_join_colname='company')\n",
    "df_taxi_data = replace_label_to_master_id(df_data=df_taxi_data, df_master=df_payment_types, s_join_colname='payment_type')\n",
    "\n",
    "df_taxi_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### weather transformation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get meteo data\n",
    "url = \"https://archive-api.open-meteo.com/v1/era5\"\n",
    "di_params = {\n",
    "\"latitude\": 41.85,\n",
    "\"longitude\": -87.65,\n",
    "\"start_date\": formatted_date,\n",
    "\"end_date\": formatted_date,\n",
    "\"hourly\": \"temperature_2m,wind_speed_10m,rain,precipitation\"\n",
    "}\n",
    "response = requests.get(url, params=di_params)\n",
    "js_weather_data = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_daily_wheater_data(js_weather_data):\n",
    "    \"\"\" filter, process daily meteo wheater data\n",
    "    ver: 1.0\n",
    "    Params:\n",
    "        weather_data (json): wheater data to pocessing\n",
    "    Returns:\n",
    "        df: processed wheater data\n",
    "    requirements:\n",
    "        module(s): pandas\n",
    "    \"\"\"\n",
    "    # selected data to dataFrame\n",
    "    di_weather_data_filt = {\n",
    "        \"datetime\": js_weather_data[\"hourly\"][\"time\"],\n",
    "        \"tempretaure\": js_weather_data[\"hourly\"][\"temperature_2m\"],\n",
    "        \"wind_speed\": js_weather_data[\"hourly\"][\"wind_speed_10m\"],\n",
    "        \"rain\": js_weather_data[\"hourly\"][\"rain\"],\n",
    "        \"precipitation\": js_weather_data[\"hourly\"][\"precipitation\"]\n",
    "    }\n",
    "    df_wether_data = pd.DataFrame(di_weather_data_filt)\n",
    "    # convert type to datetime\n",
    "    df_wether_data['datetime'] = pd.to_datetime(df_wether_data['datetime'])\n",
    "    return df_wether_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_daily_wheater_data(js_weather_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
