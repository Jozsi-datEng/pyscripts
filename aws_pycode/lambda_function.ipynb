{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from myfunc import *\n",
    "\n",
    "s_mybucket = \"jozsi-chicago-taxi-bb\"\n",
    "pathdir_proc_taxi = 'raw_data/to_process/taxi_data/'\n",
    "pathdir_processed_taxi = 'raw_data/processed/taxi_data/'\n",
    "pathdir_transf_taxi = 'transfomed_data/taxi_data/'\n",
    "pathdir_proc_wheater = 'raw_data/to_process/weather_data/'\n",
    "pathdir_processed_wheater = 'raw_data/processed/weather_data/'\n",
    "pathdir_transf_wheater = 'transfomed_data/weather_data/'\n",
    "pathdir_pay_type_master = 'transfomed_data/payment_type/'\n",
    "pathdir_company_master = 'transfomed_data/company/'\n",
    "pathdir_prev_masters = 'transfomed_data/master_table_previous_version/'\n",
    "s_pay_types = 'payment_types.csv'\n",
    "s_companies = 'companies.csv'\n",
    "s_taxi_data = 'taxi_data.csv'\n",
    "s_wheater_data = 'wheater_data.csv'\n",
    "\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    # # # main program control section  # # #\n",
    "    \n",
    "    # 1 - load and process wheater_data\n",
    "    # load weathe data to df from s3 csv\n",
    "    dic_files = load_s3_dir_files(\n",
    "        s_bucket=s_mybucket, \n",
    "        path_work = pathdir_proc_wheater, \n",
    "        s_file_sufix='json'\n",
    "        )\n",
    "    # iteration on the received dict\n",
    "    if not dic_files.keys(): print(f'There was no \".json\" file in {pathdir_proc_wheater} to process!')\n",
    "    for s_fname in dic_files.keys():\n",
    "        # 2.1 - clean & transform data\n",
    "        df_wheater_data = trasform_daily_wheater_data(dic_files[s_fname])\n",
    "\n",
    "        # 1.2 - upload results to s3\n",
    "        s_fdate = str(df_wheater_data['datetime'].iloc[0].date()) #strftime(\"%Y-%m-%d\")\n",
    "        path_file = pathdir_transf_wheater + s_wheater_data.replace('.csv', '_'+s_fdate+'.csv') # insert date to filename\n",
    "        save_df_to_s3(bucket=s_mybucket, path_file=path_file, df_data=df_wheater_data)\n",
    "        path_file = pathdir_proc_wheater + s_fname\n",
    "        path_file_new = pathdir_processed_wheater + s_fname\n",
    "        move_file_on_s3(bucket=s_mybucket, path_file=path_file,path_file_new=path_file_new)\n",
    "\n",
    "    # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "   \n",
    "    # 2 - load and process taxi data\n",
    "    # load taxi data to df from s3 csv\n",
    "    dic_files = load_s3_dir_files(\n",
    "        s_bucket=s_mybucket, \n",
    "        path_work = pathdir_proc_taxi, \n",
    "        s_file_sufix='json'\n",
    "        )\n",
    "    # iteration on the received dict\n",
    "    if not dic_files.keys(): print(f'There was no \".json\" file in {pathdir_proc_taxi} to process!')\n",
    "    for s_fname in dic_files.keys():\n",
    "        df_taxi_data = pd.DataFrame(dic_files[s_fname])\n",
    "        # 2.1 - clean & transform data\n",
    "        df_taxi_data = trasform_taxi_data(df_taxi_data)\n",
    "        # create new dim tables from taxi data\n",
    "        df_payment_types_new = df_taxi_data['payment_type'].drop_duplicates().reset_index(drop=True)\n",
    "        df_companies_new = df_taxi_data['company'].drop_duplicates().reset_index(drop=True)\n",
    "        # load master tables from s3\n",
    "        df_payment_types = load_s3_csv_to_df(\n",
    "            s_bucket=s_mybucket, \n",
    "            path_file = pathdir_pay_type_master + s_pay_types, \n",
    "            )\n",
    "        df_companies = load_s3_csv_to_df(\n",
    "            s_bucket=s_mybucket, \n",
    "            path_file = pathdir_company_master + s_companies, \n",
    "            )\n",
    "        # update master tables as needed\n",
    "        df_payment_types = master_table_update(df_master=df_payment_types, df_dim=df_payment_types_new, s_id_name='payment_type_id', s_column_label='payment_type')\n",
    "        df_companies = master_table_update(df_master=df_companies, df_dim=df_companies_new, s_id_name='company_id', s_column_label='company')\n",
    "        # join master tables ids to taxi data and remove it's original label columns\n",
    "        df_taxi_data = replace_label_to_master_id(df_data=df_taxi_data, df_master=df_payment_types, s_join_colname='payment_type')\n",
    "        df_taxi_data = replace_label_to_master_id(df_data=df_taxi_data, df_master=df_companies, s_join_colname='company')\n",
    "        # 2.2 - upload results to s3\n",
    "        # move current master tables to s3 \"previous\" folder and upload the current file\n",
    "        path_file = pathdir_pay_type_master + s_pay_types\n",
    "        path_file_new = pathdir_prev_masters + s_pay_types.replace('.csv', '_prev.csv')\n",
    "        move_file_on_s3(bucket=s_mybucket, path_file=path_file, path_file_new=path_file_new)\n",
    "        save_df_to_s3(bucket=s_mybucket, path_file=path_file, df_data=df_payment_types)\n",
    "        #\n",
    "        path_file = pathdir_company_master + s_companies\n",
    "        path_file_new = pathdir_prev_masters + s_companies.replace('.csv', '_prev.csv')\n",
    "        move_file_on_s3(bucket=s_mybucket, path_file=path_file,path_file_new=path_file_new)\n",
    "        save_df_to_s3(bucket=s_mybucket, path_file=path_file, df_data=df_companies)\n",
    "        # move taxi_data.json to s3 \"processed\" folder and upload the taxi_data.csv file to it's transform tree\n",
    "        s_fdate = str(df_taxi_data['datetime_for_weather'].iloc[0].date()) #strftime(\"%Y-%m-%d\")\n",
    "        path_file = pathdir_transf_taxi + s_taxi_data.replace('.csv', '_'+s_fdate+'.csv') # insert date to filename\n",
    "        save_df_to_s3(bucket=s_mybucket, path_file=path_file, df_data=df_taxi_data)\n",
    "        path_file = pathdir_proc_taxi + s_fname\n",
    "        path_file_new = pathdir_processed_taxi + s_fname\n",
    "        move_file_on_s3(bucket=s_mybucket, path_file=path_file,path_file_new=path_file_new)\n",
    "\n",
    "    return True\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
