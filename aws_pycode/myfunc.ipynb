{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "def load_s3_dir_files(s_bucket: str, path_work: str, s_file_sufix: str) -> Dict:\n",
    "    \"\"\" s3 folder downloader. Dowload files form specified folder\n",
    "    ver: 1.0\n",
    "    Params:\n",
    "        s_bucket (str): s3 bucket name\n",
    "        path_work (str): work dir path\n",
    "    Returns:\n",
    "        dict: found records {filename: file}\n",
    "    requirements:\n",
    "        module(s): boto3, json\n",
    "    \"\"\"\n",
    "    dic_res = {}\n",
    "    for s_fname in s3.list_objects(Bucket=s_bucket, Prefix=path_work)['Contents']:\n",
    "        s_key = s_fname['Key']\n",
    "        if s_key.split(\"/\")[-1].strip() != \"\":\n",
    "            if s_key.split(\".\")[1] == s_file_sufix:\n",
    "                response = s3.get_object(Bucket=s_bucket, Key=s_key)\n",
    "                content = response['Body']\n",
    "                js_data = json.loads(content.read())\n",
    "                dic_res.update({s_key.split(\"/\")[-1].strip(): js_data})\n",
    "    return dic_res\n",
    "\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "def load_s3_csv_to_df(s_bucket: str, path_file: str) -> Union[pd.DataFrame, None]:\n",
    "    \"\"\"Load CSV file from S3 into a pandas DataFrame\n",
    "    ver: 1.0\n",
    "    Params:\n",
    "        s_bucket (str): The name of the S3 bucket\n",
    "        s_path_file (str): The file path inside the S3 bucket\n",
    "    Return:\n",
    "        (df): DataFrame with CSV content\n",
    "    Requirements:\n",
    "        module(s): boto3, pandas, StringIO\n",
    "    \"\"\"\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    try:\n",
    "        # Get the object from S3\n",
    "        obj_csv = s3.get_object(Bucket=s_bucket, Key=path_file)\n",
    "        obj_csv_data = obj_csv[\"Body\"].read().decode(\"utf-8\")\n",
    "        # Read the CSV data into a pandas DataFrame\n",
    "        df_data = pd.read_csv(StringIO(obj_csv_data), delimiter=';')\n",
    "        return df_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "def save_df_to_s3(bucket: str, path_file: str, df_data: pd.DataFrame) -> Union[str, None]:\n",
    "    \"\"\"Save a DataFrame to S3 as a CSV file\n",
    "    ver: 1.0\n",
    "    Params:\n",
    "        path_file (str): The full file path, including the filename\n",
    "        df_data (df): The DataFrame to be saved to S3\n",
    "        bucket (str): The name of the S3 bucket\n",
    "    Return:\n",
    "        str: message\n",
    "    Requirements:\n",
    "        module(s): boto3, pandas\n",
    "    \"\"\"\n",
    "    s3 = boto3.client(\"s3\") \n",
    "    try:\n",
    "        # Convert DataFrame to CSV in memory\n",
    "        buffer = StringIO()\n",
    "        df_data.to_csv(buffer, sep=\";\", index=False)\n",
    "        buffer.seek(0)\n",
    "        # Save the CSV to S3\n",
    "        s3.put_object(Bucket=bucket, Key=path_file, Body=buffer.getvalue())\n",
    "        print (f\"File saved {path_file} successfully\")\n",
    "    except Exception as e:\n",
    "        print (f\"Error: {str(e)}\")\n",
    "\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "def move_file_on_s3(bucket: str, path_file: str, path_file_new: str) -> None:\n",
    "    \"\"\"Move file in S3 (copy and then delete original)\n",
    "    ver: 1.0\n",
    "    Params:\n",
    "        path_file (str): full file path in the source S3 bucket\n",
    "        path_file_new (str): full file path with new filename in the destination S3 bucket\n",
    "        bucket (str): name of the S3 bucket\n",
    "    return: message (str)\n",
    "    requirements:\n",
    "        module(s): boto3\n",
    "    \"\"\"\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    # Copy the file to the new location\n",
    "    s3.copy_object(\n",
    "        Bucket=bucket,\n",
    "        CopySource={\"Bucket\": bucket, \"Key\": path_file},\n",
    "        Key=path_file_new\n",
    "    )\n",
    "    # Delete the original file (not always necessary, but it will be general)\n",
    "    s3.delete_object(\n",
    "        Bucket=bucket,\n",
    "        Key=path_file\n",
    "    )\n",
    "    print (f\"File moved from {path_file} to {path_file_new}\")\n",
    "\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "def trasform_daily_wheater_data(js_weather_data: Dict) -> pd.DataFrame:\n",
    "    \"\"\" filter, process daily wheater data\n",
    "    ver: 1.0\n",
    "    Params:\n",
    "        weather_data (json): wheater data to pocessing\n",
    "    Returns:\n",
    "        df: processed wheater data\n",
    "    requirements:\n",
    "        module(s): pandas\n",
    "    \"\"\"\n",
    "    # selected data to dataFrame\n",
    "    di_weather_data_filt = {\n",
    "        \"datetime\": js_weather_data[\"hourly\"][\"time\"],\n",
    "        \"tempretaure\": js_weather_data[\"hourly\"][\"temperature_2m\"],\n",
    "        \"wind_speed\": js_weather_data[\"hourly\"][\"wind_speed_10m\"],\n",
    "        \"rain\": js_weather_data[\"hourly\"][\"rain\"],\n",
    "        \"precipitation\": js_weather_data[\"hourly\"][\"precipitation\"]\n",
    "    }\n",
    "    df_wether_data = pd.DataFrame(di_weather_data_filt)\n",
    "    # convert type to datetime\n",
    "    df_wether_data['datetime'] = pd.to_datetime(df_wether_data['datetime'])\n",
    "    return df_wether_data\n",
    "    \n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "def trasform_taxi_data(df_taxi_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" taxi_data_transformations\n",
    "    ver: v1.1\n",
    "    Params:\n",
    "        df_taxi_data (df): original daly taxi data\n",
    "    Returns:\n",
    "        df: cleaned & trasform taxi data\n",
    "    requirements:\n",
    "        module(s): pandas\n",
    "    \"\"\"\n",
    "    # drop unnecessary columns\n",
    "    df_taxi_data.drop([\"pickup_census_tract\", \"dropoff_census_tract\"], axis=1, inplace=True)\n",
    "    df_taxi_data.drop([\"pickup_centroid_location\", \"dropoff_centroid_location\"], axis=1, inplace=True)\n",
    "    # drop rows that have missings\n",
    "    df_taxi_data.dropna (inplace=True)\n",
    "    # renaming cols\n",
    "    di_col_old_new = {\n",
    "        \"pickup_community_area\": \"pickup_community_area_id\", \"dropoff_community_area\": \"dropoff_community_area_id\"}\n",
    "    df_taxi_data.rename (columns=di_col_old_new, inplace=True)\n",
    "    # create helper column\n",
    "    df_taxi_data[\"datetime_for_weather\"] = pd.to_datetime(df_taxi_data[\"trip_start_timestamp\"])\n",
    "    # time rounding to date\n",
    "    df_taxi_data[\"datetime_for_weather\"] = df_taxi_data[\"datetime_for_weather\"].dt.floor(\"h\")\n",
    "    # create dict with types def\n",
    "    di_df_col_type = {\n",
    "        \"trip_id\": \"object\",\n",
    "        \"taxi_id\": \"object\",\n",
    "        \"trip_start_timestamp\": \"datetime64[ns]\",\n",
    "        \"trip_end_timestamp\": \"datetime64[ns]\",\n",
    "        \"trip_seconds\": \"int32\",\n",
    "        \"trip_miles\": \"float64\",\n",
    "        \"pickup_community_area_id\": \"int8\",\n",
    "        \"dropoff_community_area_id\": \"int8\",\n",
    "        \"fare\": \"float64\",\n",
    "        \"tips\": \"float64\",\n",
    "        \"tolls\": \"float64\",\n",
    "        \"extras\": \"float64\",\n",
    "        \"trip_total\": \"float64\",\n",
    "        \"payment_type\": \"object\",\n",
    "        \"company\": \"object\",\n",
    "        \"pickup_centroid_latitude\": \"object\",\n",
    "        \"pickup_centroid_longitude\": \"object\",\n",
    "        \"dropoff_centroid_latitude\": \"object\",\n",
    "        \"dropoff_centroid_longitude\": \"object\",\n",
    "        \"datetime_for_weather\": \"datetime64[ns]\"\n",
    "    }\n",
    "    # apply to dataFrame\n",
    "    df_taxi_data = df_taxi_data.astype(di_df_col_type)\n",
    "    return df_taxi_data\n",
    "\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "def master_table_update(df_master: pd.DataFrame, df_dim: pd.DataFrame, s_id_name: str, s_column_label: str) -> pd.DataFrame:\n",
    "    \"\"\" compare master and dim table. Expand master with new items if necessary.\n",
    "    ver: 1.0\n",
    "    Params:\n",
    "        df_master (df): master data table\n",
    "        df_dim (df): Actual dim (extract) table from taxi data\n",
    "        s_id_name (str): master table id column name\n",
    "        s_column_label (str): master table label column name\n",
    "    Returns:\n",
    "        df: extended master table with new item(s)\n",
    "    requirements:\n",
    "        module(s): pandas\n",
    "    \"\"\"\n",
    "    s_my_name = 'master_table_update'\n",
    "    # we compare them with sets form\n",
    "    # se_dim = set(df_dim[s_column_label].to_list())\n",
    "    se_dim = set(df_dim.to_list())\n",
    "    se_master = set(df_master[s_column_label].to_list())\n",
    "    # make an additive list\n",
    "    ls_dim = list(se_dim - se_master)\n",
    "    # if there is'nt new element(s), return with original dataFrame\n",
    "    if not ls_dim:\n",
    "        print(f'Function {s_my_name}: No new element was added to the master table')\n",
    "        return df_master\n",
    "    # calc new id list\n",
    "    ls_master_id = list(range(len(df_master)+1,len(df_master) + len(ls_dim)+1))\n",
    "    # create a dict with the lists\n",
    "    di_company_add = {s_id_name: ls_master_id, s_column_label: ls_dim}\n",
    "    # put a dataFrame\n",
    "    df_add = pd.DataFrame(di_company_add)\n",
    "    # concat them\n",
    "    df_master = pd.concat([df_master, df_add], ignore_index=True)\n",
    "    print(f'Function {s_my_name}: master table was updated!')\n",
    "    return df_master\n",
    "\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "def replace_label_to_master_id(df_data: pd.DataFrame, df_master: pd.DataFrame, s_join_colname: str) -> pd.DataFrame:\n",
    "    \"\"\" join master table id to df_data and remove original label column\n",
    "    ver: 1.0\n",
    "    Params:\n",
    "        df_data (df): main dataFrame\n",
    "        df_master (df): master table to join\n",
    "        s_join_colname (str): join on column name\n",
    "    Returns:\n",
    "        df: main dataFrame with new id a removed label columns\n",
    "    requirements:\n",
    "        module(s): pandas\n",
    "    \"\"\"\n",
    "    # join master tables data\n",
    "    df_data = df_data.merge(df_master, on=s_join_colname)\n",
    "    # drop label coulumns\n",
    "    df_data.drop([s_join_colname], axis=1, inplace=True)\n",
    "    return df_data\n",
    "\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
